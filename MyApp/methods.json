[
  {
    "url": "https://paperswithcode.com/method/orthogonal-regularization",
    "name": "Orthogonal Regularization",
    "full_name": "Orthogonal Regularization",
    "description": "**Orthogonal Regularization** is a regularization technique for convolutional neural networks, introduced with generative modelling as the task in mind. Orthogonality is argued to be a desirable quality in ConvNet filters, partially because multiplication by an orthogonal matrix leaves the norm of the original matrix unchanged. This property is valuable in deep or recurrent networks, where repeated matrix multiplication can result in signals vanishing or exploding. To try to maintain orthogonality throughout training, Orthogonal Regularization encourages weights to be orthogonal by pushing them towards the nearest orthogonal manifold. The objective function is augmented with the cost:\r\n\r\n$$ \\mathcal{L}\\_{ortho} = \\sum\\left(|WW^{T} \u2212 I|\\right) $$\r\n\r\nWhere $\\sum$ indicates a sum across all filter banks, $W$ is a filter bank, and $I$ is the identity matrix",
    "paper": {
      "title": "Neural Photo Editing with Introspective Adversarial Networks",
      "url": "https://paperswithcode.com/paper/neural-photo-editing-with-introspective"
    },
    "introduced_year": 2000,
    "source_url": "http://arxiv.org/abs/1609.07093v3",
    "source_title": "Neural Photo Editing with Introspective Adversarial Networks",
    "code_snippet_url": "https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/nn/init.py#L423",
    "num_papers": 26,
    "collections": [
      {
        "collection": "Regularization",
        "area_id": "general",
        "area": "General"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/conditional-positional-encoding",
    "name": "Conditional Positional Encoding",
    "full_name": "Conditional Positional Encoding",
    "description": "**Conditional Positional Encoding**, or **CPE**, is a type of positional encoding for [vision transformers](https://paperswithcode.com/methods/category/vision-transformer). Unlike previous fixed or learnable positional encodings, which are predefined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE aims to generalize to the input sequences that are longer than what the model has ever seen during training. CPE can also keep the desired translation-invariance in the image classification task. CPE can be implemented with a [Position\r\nEncoding Generator](https://paperswithcode.com/method/positional-encoding-generator) (PEG) and incorporated into the current [Transformer framework](https://paperswithcode.com/methods/category/transformers).",
    "paper": {
      "title": "Conditional Positional Encodings for Vision Transformers",
      "url": "https://paperswithcode.com/paper/do-we-really-need-explicit-position-encodings"
    },
    "introduced_year": 2000,
    "source_url": "https://arxiv.org/abs/2102.10882v3",
    "source_title": "Conditional Positional Encodings for Vision Transformers",
    "code_snippet_url": "",
    "num_papers": 6,
    "collections": [
      {
        "collection": "Position Embeddings",
        "area_id": "general",
        "area": "General"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/cw-cnn-cw-at",
    "name": "CW-CNN & CW-AT",
    "full_name": "CW-Complex Attention Mechanism",
    "description": "Please enter a description about the method here",
    "paper": {
      "title": "CW-CNN & CW-AN: Convolutional Networks and Attention Networks for CW-Complexes",
      "url": "https://paperswithcode.com/paper/cw-cnn-cw-an-convolutional-networks-and"
    },
    "introduced_year": 2000,
    "source_url": "https://arxiv.org/abs/2408.16686v2",
    "source_title": "CW-CNN & CW-AN: Convolutional Networks and Attention Networks for CW-Complexes",
    "code_snippet_url": null,
    "num_papers": 1,
    "collections": [
      {
        "collection": "Attention",
        "area_id": "general",
        "area": "General"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/gagnn",
    "name": "GAGNN",
    "full_name": "Group-Aware Neural Network",
    "description": "**GAGNN**, or **Group-aware Graph Neural Network**, is a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups.",
    "paper": {
      "title": "Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting",
      "url": "https://paperswithcode.com/paper/group-aware-graph-neural-network-for"
    },
    "introduced_year": 2000,
    "source_url": "https://arxiv.org/abs/2108.12238v1",
    "source_title": "Group-Aware Graph Neural Network for Nationwide City Air Quality Forecasting",
    "code_snippet_url": null,
    "num_papers": 1,
    "collections": [
      {
        "collection": "Graph Models",
        "area_id": "graphs",
        "area": "Graphs"
      },
      {
        "collection": "Air Quality Forecasting",
        "area_id": "general",
        "area": "General"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/hfpso",
    "name": "HFPSO",
    "full_name": "Hybrid Firefly and Particle Swarm Optimization",
    "description": "**Hybrid Firefly and Particle Swarm Optimization (HFPSO)** is a metaheuristic optimization algorithm that combines strong points of firefly and particle swarm optimization. HFPSO tries to determine the start of the local search process properly by checking the previous global best fitness values.\r\n\r\n[Click Here for the Paper](https://www.sciencedirect.com/science/article/abs/pii/S156849461830084X)\r\n\r\n[Codes (MATLAB)](https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso)",
    "paper": null,
    "introduced_year": 2000,
    "source_url": null,
    "source_title": null,
    "code_snippet_url": "https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso",
    "num_papers": 0,
    "collections": [
      {
        "collection": "Optimization",
        "area_id": "general",
        "area": "General"
      },
      {
        "collection": "Hybrid Optimization",
        "area_id": "general",
        "area": "General"
      },
      {
        "collection": "Heuristic Search Algorithms",
        "area_id": "reinforcement-learning",
        "area": "Reinforcement Learning"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/dafne",
    "name": "DAFNe",
    "full_name": "DAFNe",
    "description": "**DAFNe** is a dense one-stage anchor-free deep model for oriented object detection. It is a deep neural network that performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, it reduces the prediction complexity by refraining from employing bounding box anchors. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. Moreover, it introduces an orientation-aware generalization of the center-ness function to arbitrary quadrilaterals that takes into account the object's orientation and that, accordingly, accurately down-weights low-quality predictions",
    "paper": {
      "title": "DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection",
      "url": "https://paperswithcode.com/paper/dafne-a-one-stage-anchor-free-deep-model-for"
    },
    "introduced_year": 2000,
    "source_url": "https://arxiv.org/abs/2109.06148v4",
    "source_title": "DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection",
    "code_snippet_url": null,
    "num_papers": 2,
    "collections": [
      {
        "collection": "Object Detection Models",
        "area_id": "computer-vision",
        "area": "Computer Vision"
      },
      {
        "collection": "Oriented Object Detection Models",
        "area_id": "computer-vision",
        "area": "Computer Vision"
      }
    ]
  },
  {
    "url": "https://paperswithcode.com/method/chinese-pre-trained-unbalanced-transformer",
    "name": "Chinese Pre-trained Unbalanced Transformer",
    "full_name": "Chinese Pre-trained Unbalanced Transformer",
    "description": "**CPT**, or **Chinese Pre-trained Unbalanced Transformer**, is a pre-trained unbalanced [Transformer](https://paperswithcode.com/method/transformer) for Chinese natural language understanding (NLU) and natural language generation (NLG) tasks. CPT consists of three parts: a shared encoder, an understanding decoder, and a generation decoder. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model.",
    "paper": {
      "title": "CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation",
      "url": "https://paperswithcode.com/paper/cpt-a-pre-trained-unbalanced-transformerfor"
    },
    "introduced_year": 2000,
    "source_url": "https://arxiv.org/abs/2109.05729v4",
    "source_title": "CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation",
    "code_snippet_url": null,
    "num_papers": 1,
    "collections": [
      {
        "collection": "Transformers",
        "area_id": "natural-language-processing",
        "area": "Natural Language Processing"
      }
    ]
  }
]
